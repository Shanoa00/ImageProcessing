{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7, 10,  2,  3, 10,  2,  7,  3,  7,  2,  7, 11,  3,  7, 10,  7,\n",
       "        11,  2,  7, 10,  2, 11,  2,  0, 10,  2,  7,  2,  0,  5,  2,  7,\n",
       "        11,  0,  5,  7,  7, 11,  2,  5,  7,  8, 11,  2,  0,  7,  8,  9,\n",
       "         2,  0,  5,  8,  9,  4,  0,  5,  7,  9,  4, 12,  5,  7,  8,  4,\n",
       "        12,  6,  7,  8,  9, 12,  6,  1,  8,  9,  4,  6,  1,  9,  4, 12,\n",
       "         1,  4, 12,  6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.log(softmax_out[Y.flatten(), np.arange(Y.shape[1])] + 0.001))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    softmax_out[Y.flatten(), np.arange(m)] -= 1.0\n",
    "    dL_dZ = softmax_out\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.5521331025990563\n",
      "Cost after epoch 10: 2.551800029902041\n",
      "Cost after epoch 20: 2.5514533303291587\n",
      "Cost after epoch 30: 2.551073344437463\n",
      "Cost after epoch 40: 2.5506401608043645\n",
      "Cost after epoch 50: 2.5501323444152617\n",
      "Cost after epoch 60: 2.5495381996202786\n",
      "Cost after epoch 70: 2.5488259661037356\n",
      "Cost after epoch 80: 2.547966508155682\n",
      "Cost after epoch 90: 2.546927892625489\n",
      "Cost after epoch 100: 2.5456735761047127\n",
      "Cost after epoch 110: 2.544191329304913\n",
      "Cost after epoch 120: 2.542419262769871\n",
      "Cost after epoch 130: 2.5403033899686753\n",
      "Cost after epoch 140: 2.5377858509876545\n",
      "Cost after epoch 150: 2.534801977117158\n",
      "Cost after epoch 160: 2.531348860950112\n",
      "Cost after epoch 170: 2.527313261820005\n",
      "Cost after epoch 180: 2.5226125706575573\n",
      "Cost after epoch 190: 2.5171696449033902\n",
      "Cost after epoch 200: 2.5109105076931217\n",
      "Cost after epoch 210: 2.5039065344436127\n",
      "Cost after epoch 220: 2.4960241802283605\n",
      "Cost after epoch 230: 2.487229329150902\n",
      "Cost after epoch 240: 2.4775357619215788\n",
      "Cost after epoch 250: 2.4670019323632375\n",
      "Cost after epoch 260: 2.455947294054015\n",
      "Cost after epoch 270: 2.4443725469644306\n",
      "Cost after epoch 280: 2.432462856366265\n",
      "Cost after epoch 290: 2.420462049727646\n",
      "Cost after epoch 300: 2.4086292325088388\n",
      "Cost after epoch 310: 2.3974110578856225\n",
      "Cost after epoch 320: 2.3868054001561325\n",
      "Cost after epoch 330: 2.376917882393659\n",
      "Cost after epoch 340: 2.367803955454967\n",
      "Cost after epoch 350: 2.3594468521513363\n",
      "Cost after epoch 360: 2.351902601921512\n",
      "Cost after epoch 370: 2.3449146780799706\n",
      "Cost after epoch 380: 2.338332140510284\n",
      "Cost after epoch 390: 2.3320118212543646\n",
      "Cost after epoch 400: 2.325813129501214\n",
      "Cost after epoch 410: 2.3197183020924017\n",
      "Cost after epoch 420: 2.3135190842699416\n",
      "Cost after epoch 430: 2.307112872753105\n",
      "Cost after epoch 440: 2.300429463693341\n",
      "Cost after epoch 450: 2.2934198873435028\n",
      "Cost after epoch 460: 2.286192415317605\n",
      "Cost after epoch 470: 2.2786330619570947\n",
      "Cost after epoch 480: 2.270738703585685\n",
      "Cost after epoch 490: 2.262536250674718\n",
      "Cost after epoch 500: 2.25406570994293\n",
      "Cost after epoch 510: 2.245535475091126\n",
      "Cost after epoch 520: 2.236867188915597\n",
      "Cost after epoch 530: 2.228101278317473\n",
      "Cost after epoch 540: 2.2192964067267877\n",
      "Cost after epoch 550: 2.2105098462569828\n",
      "Cost after epoch 560: 2.201951731389839\n",
      "Cost after epoch 570: 2.193526617567347\n",
      "Cost after epoch 580: 2.1852574436144896\n",
      "Cost after epoch 590: 2.1771787377099936\n",
      "Cost after epoch 600: 2.1693190220565515\n",
      "Cost after epoch 610: 2.1618363830131098\n",
      "Cost after epoch 620: 2.1546180443727576\n",
      "Cost after epoch 630: 2.14766065531842\n",
      "Cost after epoch 640: 2.140972577462303\n",
      "Cost after epoch 650: 2.1345588561725295\n",
      "Cost after epoch 660: 2.1285301045506886\n",
      "Cost after epoch 670: 2.1227793258054652\n",
      "Cost after epoch 680: 2.117291828574848\n",
      "Cost after epoch 690: 2.112063950413866\n",
      "Cost after epoch 700: 2.1070907577452207\n",
      "Cost after epoch 710: 2.1024495423356107\n",
      "Cost after epoch 720: 2.0980506360068896\n",
      "Cost after epoch 730: 2.0938774947981686\n",
      "Cost after epoch 740: 2.0899229589579904\n",
      "Cost after epoch 750: 2.0861796501452794\n",
      "Cost after epoch 760: 2.082702329818007\n",
      "Cost after epoch 770: 2.079420877227991\n",
      "Cost after epoch 780: 2.076320904654894\n",
      "Cost after epoch 790: 2.0733953884430094\n",
      "Cost after epoch 800: 2.0706373771958613\n",
      "Cost after epoch 810: 2.0680856369666194\n",
      "Cost after epoch 820: 2.065687163477133\n",
      "Cost after epoch 830: 2.0634302866008603\n",
      "Cost after epoch 840: 2.0613088133671216\n",
      "Cost after epoch 850: 2.0593166512551977\n",
      "Cost after epoch 860: 2.057480626040751\n",
      "Cost after epoch 870: 2.055761389250864\n",
      "Cost after epoch 880: 2.0541496443962868\n",
      "Cost after epoch 890: 2.0526401085068193\n",
      "Cost after epoch 900: 2.0512276405022085\n",
      "Cost after epoch 910: 2.049930438086127\n",
      "Cost after epoch 920: 2.0487198790932384\n",
      "Cost after epoch 930: 2.0475888355182237\n",
      "Cost after epoch 940: 2.0465331108837352\n",
      "Cost after epoch 950: 2.0455486956602336\n",
      "Cost after epoch 960: 2.04464788703478\n",
      "Cost after epoch 970: 2.0438104093177305\n",
      "Cost after epoch 980: 2.04303111962354\n",
      "Cost after epoch 990: 2.042306984727914\n",
      "Cost after epoch 1000: 2.0416351595928157\n",
      "Cost after epoch 1010: 2.0410239280360494\n",
      "Cost after epoch 1020: 2.040459353138699\n",
      "Cost after epoch 1030: 2.0399379424184105\n",
      "Cost after epoch 1040: 2.0394576690081943\n",
      "Cost after epoch 1050: 2.0390166508530685\n",
      "Cost after epoch 1060: 2.0386202400364763\n",
      "Cost after epoch 1070: 2.0382591831203274\n",
      "Cost after epoch 1080: 2.037931188602011\n",
      "Cost after epoch 1090: 2.03763492133795\n",
      "Cost after epoch 1100: 2.0373691300184036\n",
      "Cost after epoch 1110: 2.0371367902125446\n",
      "Cost after epoch 1120: 2.036932054635871\n",
      "Cost after epoch 1130: 2.0367533968599933\n",
      "Cost after epoch 1140: 2.0365998502193174\n",
      "Cost after epoch 1150: 2.0364704809115\n",
      "Cost after epoch 1160: 2.036366231470294\n",
      "Cost after epoch 1170: 2.036283741757736\n",
      "Cost after epoch 1180: 2.0362219296074624\n",
      "Cost after epoch 1190: 2.0361799728375107\n",
      "Cost after epoch 1200: 2.036157054061792\n",
      "Cost after epoch 1210: 2.036152419759311\n",
      "Cost after epoch 1220: 2.036164557991112\n",
      "Cost after epoch 1230: 2.03619264971735\n",
      "Cost after epoch 1240: 2.0362359163899697\n",
      "Cost after epoch 1250: 2.0362935765089847\n",
      "Cost after epoch 1260: 2.036363573299021\n",
      "Cost after epoch 1270: 2.03644578101938\n",
      "Cost after epoch 1280: 2.036539565161633\n",
      "Cost after epoch 1290: 2.0366441761461345\n",
      "Cost after epoch 1300: 2.036758864589516\n",
      "Cost after epoch 1310: 2.0368806908892907\n",
      "Cost after epoch 1320: 2.037010546261451\n",
      "Cost after epoch 1330: 2.037147951524655\n",
      "Cost after epoch 1340: 2.0372922077801205\n",
      "Cost after epoch 1350: 2.0374426224427853\n",
      "Cost after epoch 1360: 2.0375957719821867\n",
      "Cost after epoch 1370: 2.0377532439857693\n",
      "Cost after epoch 1380: 2.0379146987510373\n",
      "Cost after epoch 1390: 2.0380795141960437\n",
      "Cost after epoch 1400: 2.0382470800273236\n",
      "Cost after epoch 1410: 2.038413833711813\n",
      "Cost after epoch 1420: 2.038581776462615\n",
      "Cost after epoch 1430: 2.038750692211811\n",
      "Cost after epoch 1440: 2.0389200552483864\n",
      "Cost after epoch 1450: 2.039089355828116\n",
      "Cost after epoch 1460: 2.0392551709821203\n",
      "Cost after epoch 1470: 2.0394196748568385\n",
      "Cost after epoch 1480: 2.0395827583396455\n",
      "Cost after epoch 1490: 2.039744004537917\n",
      "Cost after epoch 1500: 2.0399030155480293\n",
      "Cost after epoch 1510: 2.04005671766543\n",
      "Cost after epoch 1520: 2.040207264559065\n",
      "Cost after epoch 1530: 2.0403546378332225\n",
      "Cost after epoch 1540: 2.0404985353880902\n",
      "Cost after epoch 1550: 2.0406386757908126\n",
      "Cost after epoch 1560: 2.0407724752849226\n",
      "Cost after epoch 1570: 2.040901921768816\n",
      "Cost after epoch 1580: 2.0410270709156904\n",
      "Cost after epoch 1590: 2.041147733616287\n",
      "Cost after epoch 1600: 2.0412637412535166\n",
      "Cost after epoch 1610: 2.041373073093555\n",
      "Cost after epoch 1620: 2.041477450775969\n",
      "Cost after epoch 1630: 2.041576984787337\n",
      "Cost after epoch 1640: 2.0416715875984943\n",
      "Cost after epoch 1650: 2.0417611898328696\n",
      "Cost after epoch 1660: 2.0418443450753516\n",
      "Cost after epoch 1670: 2.0419224475473334\n",
      "Cost after epoch 1680: 2.041995639604746\n",
      "Cost after epoch 1690: 2.042063914463161\n",
      "Cost after epoch 1700: 2.0421272793065572\n",
      "Cost after epoch 1710: 2.042184824077547\n",
      "Cost after epoch 1720: 2.042237587678684\n",
      "Cost after epoch 1730: 2.042285719342911\n",
      "Cost after epoch 1740: 2.0423292662405217\n",
      "Cost after epoch 1750: 2.042368284473622\n",
      "Cost after epoch 1760: 2.0424023314488076\n",
      "Cost after epoch 1770: 2.042432090033754\n",
      "Cost after epoch 1780: 2.042457693349565\n",
      "Cost after epoch 1790: 2.0424792156833127\n",
      "Cost after epoch 1800: 2.042496735657569\n",
      "Cost after epoch 1810: 2.0425101983166964\n",
      "Cost after epoch 1820: 2.042519950782262\n",
      "Cost after epoch 1830: 2.0425260936289975\n",
      "Cost after epoch 1840: 2.042528707315009\n",
      "Cost after epoch 1850: 2.042527873521134\n",
      "Cost after epoch 1860: 2.0425238504721936\n",
      "Cost after epoch 1870: 2.042516684385418\n",
      "Cost after epoch 1880: 2.0425064362031513\n",
      "Cost after epoch 1890: 2.0424931810048457\n",
      "Cost after epoch 1900: 2.04247699386074\n",
      "Cost after epoch 1910: 2.0424583858294456\n",
      "Cost after epoch 1920: 2.0424371452203296\n",
      "Cost after epoch 1930: 2.042413295436242\n",
      "Cost after epoch 1940: 2.042386903992658\n",
      "Cost after epoch 1950: 2.042358038757554\n",
      "Cost after epoch 1960: 2.042327417995872\n",
      "Cost after epoch 1970: 2.0422946177848433\n",
      "Cost after epoch 1980: 2.0422596325404117\n",
      "Cost after epoch 1990: 2.0422225265119445\n",
      "Cost after epoch 2000: 2.042183365552982\n",
      "Cost after epoch 2010: 2.042143039154712\n",
      "Cost after epoch 2020: 2.0421009558502687\n",
      "Cost after epoch 2030: 2.0420570922927097\n",
      "Cost after epoch 2040: 2.0420115162333388\n",
      "Cost after epoch 2050: 2.0419642984028834\n",
      "Cost after epoch 2060: 2.0419164661351696\n",
      "Cost after epoch 2070: 2.0418673025370326\n",
      "Cost after epoch 2080: 2.0418167770290863\n",
      "Cost after epoch 2090: 2.0417649665737403\n",
      "Cost after epoch 2100: 2.04171195203268\n",
      "Cost after epoch 2110: 2.041658861892284\n",
      "Cost after epoch 2120: 2.041604893386585\n",
      "Cost after epoch 2130: 2.0415500164174945\n",
      "Cost after epoch 2140: 2.0414943196301554\n",
      "Cost after epoch 2150: 2.0414378957363484\n",
      "Cost after epoch 2160: 2.0413819313245467\n",
      "Cost after epoch 2170: 2.0413255754336195\n",
      "Cost after epoch 2180: 2.0412688025760946\n",
      "Cost after epoch 2190: 2.041211711620518\n",
      "Cost after epoch 2200: 2.0411544048859236\n",
      "Cost after epoch 2210: 2.041098077797937\n",
      "Cost after epoch 2220: 2.04104186754885\n",
      "Cost after epoch 2230: 2.040985754107887\n",
      "Cost after epoch 2240: 2.040929841683018\n",
      "Cost after epoch 2250: 2.040874236699321\n",
      "Cost after epoch 2260: 2.0408200907271214\n",
      "Cost after epoch 2270: 2.0407665640666144\n",
      "Cost after epoch 2280: 2.0407136406602744\n",
      "Cost after epoch 2290: 2.0406614230053126\n",
      "Cost after epoch 2300: 2.0406100142418198\n",
      "Cost after epoch 2310: 2.040560470351914\n",
      "Cost after epoch 2320: 2.0405120075262686\n",
      "Cost after epoch 2330: 2.040464611062881\n",
      "Cost after epoch 2340: 2.0404183740738917\n",
      "Cost after epoch 2350: 2.0403733887000315\n",
      "Cost after epoch 2360: 2.0403305693709615\n",
      "Cost after epoch 2370: 2.0402892174532883\n",
      "Cost after epoch 2380: 2.040249316948216\n",
      "Cost after epoch 2390: 2.040210944709937\n",
      "Cost after epoch 2400: 2.0401741752242186\n",
      "Cost after epoch 2410: 2.0401397453681587\n",
      "Cost after epoch 2420: 2.0401070659132103\n",
      "Cost after epoch 2430: 2.040076117664279\n",
      "Cost after epoch 2440: 2.04004695618761\n",
      "Cost after epoch 2450: 2.0400196336751892\n",
      "Cost after epoch 2460: 2.0399946862900307\n",
      "Cost after epoch 2470: 2.039971651940164\n",
      "Cost after epoch 2480: 2.0399505074806403\n",
      "Cost after epoch 2490: 2.0399312845466353\n",
      "Cost after epoch 2500: 2.039914010863009\n",
      "Cost after epoch 2510: 2.0398990127926138\n",
      "Cost after epoch 2520: 2.039885965593743\n",
      "Cost after epoch 2530: 2.0398748425915376\n",
      "Cost after epoch 2540: 2.039865651297541\n",
      "Cost after epoch 2550: 2.0398583952393694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2560: 2.0398531956009394\n",
      "Cost after epoch 2570: 2.039849867273488\n",
      "Cost after epoch 2580: 2.0398483814027206\n",
      "Cost after epoch 2590: 2.0398487233192104\n",
      "Cost after epoch 2600: 2.0398508746870814\n",
      "Cost after epoch 2610: 2.039854768150822\n",
      "Cost after epoch 2620: 2.0398603528371795\n",
      "Cost after epoch 2630: 2.0398675995997233\n",
      "Cost after epoch 2640: 2.039876475111716\n",
      "Cost after epoch 2650: 2.039886942979935\n",
      "Cost after epoch 2660: 2.0398987733816494\n",
      "Cost after epoch 2670: 2.0399120374560056\n",
      "Cost after epoch 2680: 2.039926707759089\n",
      "Cost after epoch 2690: 2.039942736762036\n",
      "Cost after epoch 2700: 2.0399600746320776\n",
      "Cost after epoch 2710: 2.039978361563965\n",
      "Cost after epoch 2720: 2.039997772976785\n",
      "Cost after epoch 2730: 2.0400182848572292\n",
      "Cost after epoch 2740: 2.0400398402384208\n",
      "Cost after epoch 2750: 2.0400623806644522\n",
      "Cost after epoch 2760: 2.040085452207203\n",
      "Cost after epoch 2770: 2.040109312918687\n",
      "Cost after epoch 2780: 2.0401339434165218\n",
      "Cost after epoch 2790: 2.0401592818577927\n",
      "Cost after epoch 2800: 2.040185265690211\n",
      "Cost after epoch 2810: 2.0402113833610445\n",
      "Cost after epoch 2820: 2.0402379516773723\n",
      "Cost after epoch 2830: 2.040264956432275\n",
      "Cost after epoch 2840: 2.040292334908877\n",
      "Cost after epoch 2850: 2.0403200243660096\n",
      "Cost after epoch 2860: 2.0403474905255035\n",
      "Cost after epoch 2870: 2.040375084445976\n",
      "Cost after epoch 2880: 2.0404027969794187\n",
      "Cost after epoch 2890: 2.040430567776268\n",
      "Cost after epoch 2900: 2.0404583370209366\n",
      "Cost after epoch 2910: 2.0404855793534766\n",
      "Cost after epoch 2920: 2.0405126563546734\n",
      "Cost after epoch 2930: 2.040539563244896\n",
      "Cost after epoch 2940: 2.040566244455451\n",
      "Cost after epoch 2950: 2.040592645373028\n",
      "Cost after epoch 2960: 2.040618276861627\n",
      "Cost after epoch 2970: 2.04064348918528\n",
      "Cost after epoch 2980: 2.0406682808101566\n",
      "Cost after epoch 2990: 2.040692602565889\n",
      "Cost after epoch 3000: 2.0407164065276504\n",
      "Cost after epoch 3010: 2.040739262177483\n",
      "Cost after epoch 3020: 2.0407614893157686\n",
      "Cost after epoch 3030: 2.0407830882618883\n",
      "Cost after epoch 3040: 2.0408040171583552\n",
      "Cost after epoch 3050: 2.0408242355672868\n",
      "Cost after epoch 3060: 2.0408433889121316\n",
      "Cost after epoch 3070: 2.0408617505688396\n",
      "Cost after epoch 3080: 2.0408793212030987\n",
      "Cost after epoch 3090: 2.040896066605591\n",
      "Cost after epoch 3100: 2.0409119540642546\n",
      "Cost after epoch 3110: 2.0409267173509047\n",
      "Cost after epoch 3120: 2.0409405708628623\n",
      "Cost after epoch 3130: 2.0409535141238058\n",
      "Cost after epoch 3140: 2.0409655204597423\n",
      "Cost after epoch 3150: 2.0409765646959244\n",
      "Cost after epoch 3160: 2.040986476816205\n",
      "Cost after epoch 3170: 2.0409954030871393\n",
      "Cost after epoch 3180: 2.0410033405230323\n",
      "Cost after epoch 3190: 2.0410102695482193\n",
      "Cost after epoch 3200: 2.0410161720321813\n",
      "Cost after epoch 3210: 2.0410209780019635\n",
      "Cost after epoch 3220: 2.0410247596673305\n",
      "Cost after epoch 3230: 2.0410275103573174\n",
      "Cost after epoch 3240: 2.0410292169421513\n",
      "Cost after epoch 3250: 2.0410298676442924\n",
      "Cost after epoch 3260: 2.0410294929154396\n",
      "Cost after epoch 3270: 2.0410280879130958\n",
      "Cost after epoch 3280: 2.041025641337926\n",
      "Cost after epoch 3290: 2.041022145728276\n",
      "Cost after epoch 3300: 2.0410175948573968\n",
      "Cost after epoch 3310: 2.0410121171383144\n",
      "Cost after epoch 3320: 2.0410056302418442\n",
      "Cost after epoch 3330: 2.040998117546876\n",
      "Cost after epoch 3340: 2.0409895764278296\n",
      "Cost after epoch 3350: 2.040980005363838\n",
      "Cost after epoch 3360: 2.040969626011589\n",
      "Cost after epoch 3370: 2.0409582802762865\n",
      "Cost after epoch 3380: 2.040945945769187\n",
      "Cost after epoch 3390: 2.040932623867915\n",
      "Cost after epoch 3400: 2.040918316920597\n",
      "Cost after epoch 3410: 2.0409033334068427\n",
      "Cost after epoch 3420: 2.0408874429306967\n",
      "Cost after epoch 3430: 2.0408706171215507\n",
      "Cost after epoch 3440: 2.0408528605645793\n",
      "Cost after epoch 3450: 2.0408341786837134\n",
      "Cost after epoch 3460: 2.0408149591542393\n",
      "Cost after epoch 3470: 2.0407949040898656\n",
      "Cost after epoch 3480: 2.0407739791304857\n",
      "Cost after epoch 3490: 2.040752191335592\n",
      "Cost after epoch 3500: 2.040729548478643\n",
      "Cost after epoch 3510: 2.0407065090305414\n",
      "Cost after epoch 3520: 2.040682713404805\n",
      "Cost after epoch 3530: 2.0406581214205484\n",
      "Cost after epoch 3540: 2.040632741955686\n",
      "Cost after epoch 3550: 2.0406065844871466\n",
      "Cost after epoch 3560: 2.0405801694766805\n",
      "Cost after epoch 3570: 2.040553082079162\n",
      "Cost after epoch 3580: 2.0405252766055826\n",
      "Cost after epoch 3590: 2.0404967631807205\n",
      "Cost after epoch 3600: 2.0404675524248423\n",
      "Cost after epoch 3610: 2.0404382178848066\n",
      "Cost after epoch 3620: 2.0404082962656633\n",
      "Cost after epoch 3630: 2.0403777367946234\n",
      "Cost after epoch 3640: 2.040346550357201\n",
      "Cost after epoch 3650: 2.04031474824298\n",
      "Cost after epoch 3660: 2.0402829483204123\n",
      "Cost after epoch 3670: 2.040250645781772\n",
      "Cost after epoch 3680: 2.040217785276015\n",
      "Cost after epoch 3690: 2.0401843780456916\n",
      "Cost after epoch 3700: 2.040150435658045\n",
      "Cost after epoch 3710: 2.0401166118701\n",
      "Cost after epoch 3720: 2.0400823672314177\n",
      "Cost after epoch 3730: 2.0400476423717593\n",
      "Cost after epoch 3740: 2.0400124485636812\n",
      "Cost after epoch 3750: 2.0399767973365748\n",
      "Cost after epoch 3760: 2.039941370373135\n",
      "Cost after epoch 3770: 2.039905600229124\n",
      "Cost after epoch 3780: 2.0398694241034767\n",
      "Cost after epoch 3790: 2.039832853041013\n",
      "Cost after epoch 3800: 2.0397958982862257\n",
      "Cost after epoch 3810: 2.039759262052461\n",
      "Cost after epoch 3820: 2.039722355212355\n",
      "Cost after epoch 3830: 2.0396851121311745\n",
      "Cost after epoch 3840: 2.039647543429389\n",
      "Cost after epoch 3850: 2.039609659879668\n",
      "Cost after epoch 3860: 2.039572177461335\n",
      "Cost after epoch 3870: 2.0395344912519127\n",
      "Cost after epoch 3880: 2.0394965333721244\n",
      "Cost after epoch 3890: 2.039458313874262\n",
      "Cost after epoch 3900: 2.039419842923957\n",
      "Cost after epoch 3910: 2.0393818441655536\n",
      "Cost after epoch 3920: 2.039343702290451\n",
      "Cost after epoch 3930: 2.0393053477426117\n",
      "Cost after epoch 3940: 2.039266789907022\n",
      "Cost after epoch 3950: 2.039228038250658\n",
      "Cost after epoch 3960: 2.0391898186556237\n",
      "Cost after epoch 3970: 2.039151510324321\n",
      "Cost after epoch 3980: 2.0391130425603112\n",
      "Cost after epoch 3990: 2.0390744240192675\n",
      "Cost after epoch 4000: 2.039035663413968\n",
      "Cost after epoch 4010: 2.038997484102315\n",
      "Cost after epoch 4020: 2.038959264169414\n",
      "Cost after epoch 4030: 2.038920932276125\n",
      "Cost after epoch 4040: 2.038882496317039\n",
      "Cost after epoch 4050: 2.0388439642244616\n",
      "Cost after epoch 4060: 2.038806052713047\n",
      "Cost after epoch 4070: 2.03876814259873\n",
      "Cost after epoch 4080: 2.0387301623539282\n",
      "Cost after epoch 4090: 2.0386921191045135\n",
      "Cost after epoch 4100: 2.0386540199992664\n",
      "Cost after epoch 4110: 2.0386165716008753\n",
      "Cost after epoch 4120: 2.038579160794414\n",
      "Cost after epoch 4130: 2.038541716272006\n",
      "Cost after epoch 4140: 2.0385042444019414\n",
      "Cost after epoch 4150: 2.0384667515644184\n",
      "Cost after epoch 4160: 2.038429931232065\n",
      "Cost after epoch 4170: 2.0383931792103347\n",
      "Cost after epoch 4180: 2.0383564247720782\n",
      "Cost after epoch 4190: 2.0383196735533535\n",
      "Cost after epoch 4200: 2.03828293119423\n",
      "Cost after epoch 4210: 2.038246875665611\n",
      "Cost after epoch 4220: 2.038210914086609\n",
      "Cost after epoch 4230: 2.038174976625062\n",
      "Cost after epoch 4240: 2.038139068220504\n",
      "Cost after epoch 4250: 2.038103193811073\n",
      "Cost after epoch 4260: 2.038068013934073\n",
      "Cost after epoch 4270: 2.0380329489909963\n",
      "Cost after epoch 4280: 2.037997930313565\n",
      "Cost after epoch 4290: 2.0379629621876414\n",
      "Cost after epoch 4300: 2.0379280488942655\n",
      "Cost after epoch 4310: 2.037893832037659\n",
      "Cost after epoch 4320: 2.037859746881742\n",
      "Cost after epoch 4330: 2.0378257261478447\n",
      "Cost after epoch 4340: 2.0377917735155364\n",
      "Cost after epoch 4350: 2.0377578926576736\n",
      "Cost after epoch 4360: 2.037724705131436\n",
      "Cost after epoch 4370: 2.037691662291525\n",
      "Cost after epoch 4380: 2.037658698434177\n",
      "Cost after epoch 4390: 2.037625816682459\n",
      "Cost after epoch 4400: 2.037593020152007\n",
      "Cost after epoch 4410: 2.0375609095791067\n",
      "Cost after epoch 4420: 2.037528953322896\n",
      "Cost after epoch 4430: 2.037497087402482\n",
      "Cost after epoch 4440: 2.0374653144349635\n",
      "Cost after epoch 4450: 2.0374336370301656\n",
      "Cost after epoch 4460: 2.037402634626521\n",
      "Cost after epoch 4470: 2.037371793222667\n",
      "Cost after epoch 4480: 2.0373410506745304\n",
      "Cost after epoch 4490: 2.0373104091432337\n",
      "Cost after epoch 4500: 2.037279870783406\n",
      "Cost after epoch 4510: 2.0372499935150024\n",
      "Cost after epoch 4520: 2.037220281367276\n",
      "Cost after epoch 4530: 2.0371906741162817\n",
      "Cost after epoch 4540: 2.0371611735156456\n",
      "Cost after epoch 4550: 2.0371317813137018\n",
      "Cost after epoch 4560: 2.0371030339100655\n",
      "Cost after epoch 4570: 2.0370744535444385\n",
      "Cost after epoch 4580: 2.0370459819690687\n",
      "Cost after epoch 4590: 2.037017620576341\n",
      "Cost after epoch 4600: 2.0369893707548146\n",
      "Cost after epoch 4610: 2.036961747566222\n",
      "Cost after epoch 4620: 2.0369342914597244\n",
      "Cost after epoch 4630: 2.0369069461958556\n",
      "Cost after epoch 4640: 2.036879712849304\n",
      "Cost after epoch 4650: 2.0368525924925414\n",
      "Cost after epoch 4660: 2.036826079184951\n",
      "Cost after epoch 4670: 2.0367997314313713\n",
      "Cost after epoch 4680: 2.03677349501188\n",
      "Cost after epoch 4690: 2.036747370723997\n",
      "Cost after epoch 4700: 2.036721359364676\n",
      "Cost after epoch 4710: 2.0366959344514766\n",
      "Cost after epoch 4720: 2.0366706722638814\n",
      "Cost after epoch 4730: 2.0366455205948903\n",
      "Cost after epoch 4740: 2.036620480002185\n",
      "Cost after epoch 4750: 2.036595551044519\n",
      "Cost after epoch 4760: 2.036571187258125\n",
      "Cost after epoch 4770: 2.036546982311888\n",
      "Cost after epoch 4780: 2.03652288598996\n",
      "Cost after epoch 4790: 2.036498898644279\n",
      "Cost after epoch 4800: 2.036475020629404\n",
      "Cost after epoch 4810: 2.0364516861386313\n",
      "Cost after epoch 4820: 2.03642850576139\n",
      "Cost after epoch 4830: 2.0364054312385886\n",
      "Cost after epoch 4840: 2.0363824627472913\n",
      "Cost after epoch 4850: 2.0363596004686206\n",
      "Cost after epoch 4860: 2.0363372599498595\n",
      "Cost after epoch 4870: 2.0363150681667608\n",
      "Cost after epoch 4880: 2.0362929787722543\n",
      "Cost after epoch 4890: 2.0362709917962705\n",
      "Cost after epoch 4900: 2.036249107274101\n",
      "Cost after epoch 4910: 2.0362277228456906\n",
      "Cost after epoch 4920: 2.036206481289564\n",
      "Cost after epoch 4930: 2.036185338117639\n",
      "Cost after epoch 4940: 2.0361642932374666\n",
      "Cost after epoch 4950: 2.036143346563113\n",
      "Cost after epoch 4960: 2.036122878593167\n",
      "Cost after epoch 4970: 2.0361025472899894\n",
      "Cost after epoch 4980: 2.036082309964994\n",
      "Cost after epoch 4990: 2.0360621664252907\n",
      "training time: 0:00:00.389809\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investing's neighbor words: ['the', 'stock', 'beating', \"loser's\"]\n",
      "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
      "is's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
      "beating's neighbor words: ['market', 'stock', 'costs', 'investing']\n",
      "loser's's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
      "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
      "stock's neighbor words: ['investing', 'a', 'is', 'market']\n",
      "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n",
      "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
      "a's neighbor words: ['game', 'market', \"loser's\", 'a']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
